# FinRLExtention Configuration

# Gemini API Configuration
gemini:
  # API key for Google Gemini - replace with your actual key
  api_key: "ignore"  # Set your API key here or use environment variable

  # Model to use (options: gemini-1.5-pro, gemini-1.5-flash, gemini-1.0-pro)
  model: "gemini-1.5-pro"

  # Temperature for response generation (0.0 - 1.0)
  temperature: 0.7

  # Maximum tokens in the response
  max_output_tokens: 8192

# MCP Server Configurations
# Add your MCP servers here
mcp_servers:
  # Example server configuration:
  # - name: "example-server"
  #   command: "python"
  #   args: ["-m", "example_mcp_server"]
  #   env:
  #     SOME_VAR: "value"

  # Filesystem server example:
  # - name: "filesystem"
  #   command: "npx"
  #   args: ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/directory"]
